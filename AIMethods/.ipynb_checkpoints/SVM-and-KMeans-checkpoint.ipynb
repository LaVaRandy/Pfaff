{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Support Vector Machines and KMeans clustering\n",
    "\n",
    "This is an extremely brief introduction to \n",
    "\n",
    "- Support Vector Machines (Splitting datasets along a hyperplane -> Supervised Learning)\n",
    "- KMeans clustering (Clustering datasets into similar groups -> Unsupervised Learning)\n",
    "\n",
    "For both techniques, the well documented and usable implementations in scikit learn are used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard imports\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "# Scikit learn imports\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import datasets\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load DB open container dataset\n",
    "\n",
    "Contains position and state data from containers in transport. Since this gives us data with label and x-y-position, we can use them for our first AI software!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "df = pd.read_csv('http://download-data.deutschebahn.com/static/datasets/sensordaten_schenker/161209_Schenker_Sensordaten.csv')\n",
    "# First line contains comments\n",
    "df.drop(0, inplace = True)\n",
    "# inspect\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dataset is a good example for realworld datasets with missing data and string formatted numbers. So we proceed with data cleaning:\n",
    "\n",
    "- Remove NaN\n",
    "- Convert strings to numbers\n",
    "\n",
    "The ```describe()``` method gives a good impression whether we succeeded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(subset=['Latitude', 'Longitude', 'Humidity', 'Temperature'], inplace = True)\n",
    "df = df[['Latitude', 'Longitude', 'Humidity', 'Temperature']].apply(pd.to_numeric)\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Further data inspection:\n",
    "\n",
    "Scatter plot of position with 'Temperature'-coloured marker, no map background:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.plot(x = 'Longitude', y = 'Latitude', kind = 'scatter', c = df['Temperature'], cmap = 'seismic', alpha = 0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optional: Scatter plot on map background"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [ dict(\n",
    "        type = 'scattergeo',\n",
    "        #locationmode = 'Germany',\n",
    "        lon = df['Longitude'],\n",
    "        lat = df['Latitude'],\n",
    "        text = df['Temperature'],\n",
    "        mode = 'markers',\n",
    "        marker = dict(\n",
    "            size = 0.1*df['Humidity'],\n",
    "            opacity = 0.8,\n",
    "            #reversescale = True,\n",
    "            autocolorscale = False,\n",
    "            #symbol = 'square',\n",
    "            colorscale = 'Jet',\n",
    "            color = df['Temperature'],\n",
    "            #cmax = df['Temperature'].max(),\n",
    "            colorbar=dict(thickness=20)\n",
    "        ))]\n",
    "\n",
    "layout = dict(\n",
    "        title = 'Observed container temperatures',\n",
    "        colorbar = dict(),\n",
    "        geo = dict(\n",
    "            showland = True,\n",
    "            landcolor = \"rgb(250, 250, 250)\",\n",
    "            subunitcolor = \"rgb(217, 217, 217)\",\n",
    "            countrycolor = \"rgb(217, 217, 217)\",\n",
    "            countrywidth = 0.5,\n",
    "            subunitwidth = 0.5,\n",
    "            lonaxis = dict(\n",
    "                showgrid = True,\n",
    "                gridwidth = 0.5,\n",
    "                range= [ df['Longitude'].min(), df['Longitude'].max() ],\n",
    "                dtick = 5\n",
    "            ),\n",
    "            lataxis = dict (\n",
    "                showgrid = True,\n",
    "                gridwidth = 0.5,\n",
    "                range= [ df['Latitude'].min(), df['Latitude'].max() ],\n",
    "                dtick = 5\n",
    "            )\n",
    "        ),\n",
    "    )\n",
    "\n",
    "fig = dict( data=data, layout=layout )\n",
    "po.plot(fig, validate=False, filename='d3-container.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Primer on AI techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.lib.display import YouTubeVideo\n",
    "YouTubeVideo('2veirpJPNa8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train SVM\n",
    "\n",
    "Let's try to predict frost since this may ruin some goods. To achieve this, we need to:\n",
    "\n",
    "- Split the dataset into training and test data\n",
    "\n",
    "    - It is important to test on unseen data\n",
    "    \n",
    "- Scale data\n",
    "\n",
    "    - Equally important to scale the data in order to normalise variance\n",
    "    \n",
    "- Fit SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For splitting the dataset, we can use the ```train_test_split``` function of scikit learn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "# Data selection\n",
    "X = df[['Latitude', 'Longitude']]\n",
    "# Label target data (we need 2 classes!) \n",
    "y = (df['Temperature'] <0)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.33, random_state=42) # one third of the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The scaler of choice is the standard scaler, which needs to be trained itself (which however can be done in one step with the transformation using ```fit_transform```):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialise scaler\n",
    "scaler = StandardScaler()\n",
    "# Scale data\n",
    "X_train_scale = scaler.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to train the SVM, we initialise it, in the case at hand with two parameters:\n",
    "\n",
    "- C: the \"penalty\" of misclassifications,\n",
    "\n",
    "    - lower C values provide smooth classifiers\n",
    "    - higher C values provide more strict classification\n",
    "    \n",
    "- class_weight: in the data present, we have an unbalanced proportion of true (= frost) and false, so we can give more weight to true values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialise support vector machine\n",
    "svm_clf = SVC(C = 0.1, class_weight = {True: 10, False:1})\n",
    "# Fit SVM\n",
    "svm_clf.fit(X_train_scale, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot decision surface of classifier\n",
    "\n",
    "In order to print the decision surface of our classifier, we need to generate gridded data and plot these on the x-y-plane."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set()\n",
    "# Plot 3D scatter with decision boundary\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "x = X_train['Longitude']\n",
    "y = X_train['Latitude']\n",
    "c = y_train*1.0\n",
    "\n",
    "xx = np.linspace(min(X_train_scale[:,0]), max(X_train_scale[:,0]), 50)\n",
    "yy = np.linspace(min(X_train_scale[:,1]), max(X_train_scale[:,1]), 50)\n",
    "[Y, X] = np.meshgrid(xx, yy)\n",
    "Z = svm_clf.predict(np.c_[X.ravel(), Y.ravel()])\n",
    "Z = Z.reshape(X.shape)\n",
    "xx = np.linspace(min(x), max(x), 50)\n",
    "yy = np.linspace(min(y), max(y), 50)\n",
    "[X, Y] = np.meshgrid(xx, yy)\n",
    "\n",
    "with sns.axes_style('white'):\n",
    "    fig = plt.figure(figsize=(16, 6))\n",
    "    ax = fig.add_subplot(111)\n",
    "    ax.contourf(X, Y, Z, cmap = 'Blues', alpha = 0.1)\n",
    "    ax.scatter(x, y, c = c, cmap = 'RdGy', alpha = 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise: \n",
    "\n",
    "Try with different values of C and class_weight, observe the changing decision surface."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test on unseen data\n",
    "\n",
    "As discussed above, we need to test whether the trained algorithm works on data not used for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_scale = scaler.transform(X_test)\n",
    "# Predict \n",
    "y_pred = svm_clf.predict(X_test_scale)\n",
    "# Plot\n",
    "X_test.plot(x = 'Longitude', y = 'Latitude', kind = 'scatter', c=y_pred.astype(int), cmap = 'RdYlGn')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While the above plot appears to be credible, we should inspect confusion matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_mat = confusion_matrix(1.0*y_test, 1.0*y_pred)\n",
    "plt.imshow(conf_mat, cmap = plt.cm.Blues, alpha = 0.3)\n",
    "plt.colorbar()\n",
    "plt.xticks((0, 1), (\"<0\", \">=0\"))\n",
    "plt.yticks((0, 1), (\"<0\", \">=0\"))\n",
    "plt.xlabel('Predicted Class')\n",
    "plt.ylabel('True Class')\n",
    "for i,j in itertools.product((0,1), (0,1)):\n",
    "    plt.text(j,i, conf_mat[i,j], fontsize = 16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise:\n",
    "\n",
    "Repeat the SVM parameter variation and inspect the confusion matrix accordingly.\n",
    "\n",
    "What are the dangerous cases?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# KMeans clustering\n",
    "\n",
    "As opposed to the supervised learning algorithm applied above, for clustering we do not need class labels. The algorithm infers the classification from structures in the data.\n",
    "\n",
    "Let's give it a try and load some data, in the case at hand an iris flower dataset: https://scikit-learn.org/stable/datasets/toy_dataset.html#iris-dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data[:, :2]  # we only use the first two features \n",
    "#                       (see dimensionality exercise).\n",
    "y = iris.target       # the classification\n",
    "# Range\n",
    "x_min, x_max = X[:, 0].min() - .5, X[:, 0].max() + .5\n",
    "y_min, y_max = X[:, 1].min() - .5, X[:, 1].max() + .5\n",
    "\n",
    "# Fit KMeans\n",
    "km = KMeans(n_clusters = 3, \n",
    "            random_state = 42)\n",
    "km.fit(X)\n",
    "# Plotting\n",
    "#this will tell us to which cluster does the data observations belong.\n",
    "y_pred = km.labels_\n",
    "# Plot the identified clusters and compare with the answers\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16,8))\n",
    "axes[0].scatter(X[:, 0], X[:, 1], c=y, cmap = 'Set1')\n",
    "axes[1].scatter(X[:, 0], X[:, 1], c=y_pred, cmap = 'Set1')\n",
    "axes[0].set_xlabel('Sepal length')\n",
    "axes[0].set_ylabel('Sepal width')\n",
    "axes[1].set_xlabel('Sepal length')\n",
    "axes[1].set_ylabel('Sepal width')\n",
    "#axes[0].tick_params(direction='in', length=10, width=5, colors='k', labelsize=20)\n",
    "#axes[1].tick_params(direction='in', length=10, width=5, colors='k', labelsize=20)\n",
    "axes[0].set_title('True')\n",
    "axes[1].set_title('Predicted')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "- Try the example above with too few and too many clusters\n",
    "- Use ```verbose = True``` in ```Kmeans``` to see the convergence\n",
    "- Use ```np.array([[0,0],[0,0],[0,0]])``` together with verbose to comment on the convergence\n",
    "- Print the confusion matrix (no need to plot, however feel free!)\n",
    "- Does increasing the dimensionality improve the prediction?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
