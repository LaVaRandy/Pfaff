% !TEX root = ../ifvWorkshopQuality.tex

\frame{\frametitle{What is machine learning?}
\lehrtext{ML is a field of study that gives computers the ability to learn without being explicitly programmed. It oftentimes sounds a bit like rocket science, however the math behind it isn't all that weird and modern software packages make it pretty accessible.}
\begin{columns}[t] 
     \begin{column}[T]{7cm} 
     	\begin{itemize}
     		\item Different paradigm:
		\begin{itemize}
		\item Derive rules from observations
		\end{itemize}
		\item Typical procedure:
		\begin{itemize}
		\item \textbf{Train:} Observe set of examples ``training data''
		\item \textbf{Fit:} Infer information of process behind data
		\item \textbf{Test:} Make prediction on unseen data ``Test data''
		\end{itemize}
		\item Variations:
		\begin{itemize}
		\item Supervised learning: provide labels with data
		\item Unsupervised learning: cluster data based on patterns
		\end{itemize}
     	\end{itemize}
     \end{column}
     	\begin{column}[T]{7cm} 
         	\begin{center}
			Traditional approach:
			\vspace{.5cm}
			\begin{tikzpicture}
 				\node[anchor = east] (d) at (0,0) {Data};
				\node[anchor = east, fill = red!20] (p) at (0,-1) {Program};
				\node[anchor = west] (o) at (4,-.5) {Output};
				\node[draw, fill = gray!20] (c) at (2,-.5) {Computer};
				\draw[->, thick] (d) -- (c);
				\draw[->, thick] (p)-- (c);
				\draw[->, thick] (c) -- (o);
			\end{tikzpicture}
			
			Machine learning approach:
			\vspace{.5cm}
			\begin{tikzpicture}
 				\node[anchor = east] (d) at (0,0) {Data};
				\node[anchor = east] (p) at (0,-1) {Output};
				\node[anchor = west,  fill = red!20] (o) at (4,-.5) {Program};
				\node[draw, fill = gray!20] (c) at (2,-.5) {Computer};
				\draw[->, thick] (d) -- (c);
				\draw[->, thick] (p)-- (c);
				\draw[->, thick] (c) -- (o);
			\end{tikzpicture}

		\end{center}
     \end{column}
 \end{columns}
}




%\section{Principal Component Analysis}
%%\frame{\frametitle{Principal Component Analysis (PCA)}
%%\framesubtitle{Using MIT OpenCourseware Slides}
%%Source:
%%\begin{center}
%%Philippe Rigollet. 18.650 Statistics for Applications . Fall 2016. Massachusetts Institute of Technology: MIT OpenCourseWare, https://ocw.mit.edu. License: Creative Commons BY-NC-SA.
%%\end{center}
%%}
%%
%%\setbeamercolor{background canvas}{bg=}
%%%\frame{
%%\includepdf[page = 2-16]{OCW/PCA.pdf}%}
%
%\frame{\frametitle{Principal Component Analysis (PCA)}
%\framesubtitle{Reduce dimensionality of data}
%\begin{columns}[t] 
%     \begin{column}[T]{7cm} 
%     	\begin{itemize}
%     		\item Sample $X_{1}, \ldots, X_{n}$ is a cloud of points in $\mathbb{R}^d$
%		\item Typically, $d$ is large
%		\item<1-> \textbf{Question:} can we project the point cloud onto a linear subspace of dimension $d' < d$ while keeping as much of the information as possible?
%		\item<2-> \textbf{Answer:} PCA achieves this by keeping important aspects of the covariance structure in orthogonal directions.
%     	\end{itemize}
%     \end{column}
%     	\begin{column}[T]{7cm} 
%         	\begin{center}
%            		\includegraphics[width=0.6\textwidth]{GaussianScatterPCA}\source{}
%        		\end{center}
%     \end{column}
% \end{columns}
%}



\section{Supervised learning}
\lehrtext{In order to give you a starting point for your own ventures, we will dive into two methods, one from the area of supervised learning, one from the unsupervised area.}
\frame{\frametitle{Support Vector Machine}
\lehrtext{Support Vector Machines allow you to eeparate data into subsets according to their $nD$-coordinates and a label, e.g. OK/NOK. Since the support vector machine uses a linear algebra approach, it is pretty robust and only needs smaller datasets.}
\begin{columns}[t] 
     \begin{column}[T]{7cm} 
     	\begin{itemize}
		\item Idea:
		\begin{itemize}
		\item Find separating hyperplane maximising the distance between borderline instances
		\item For data mixed in feature space, slack parameter (and penalty $C$) is used
		\item If no separating hyperplane can be found in feature space, dimension is increased: ``Kernel trick''
		\end{itemize}
     		\item Robust:
		\begin{itemize}
		\item High dimensionality
		\item Small datasets
		\end{itemize}
		\item Simple to complex models
     	\end{itemize}
     \end{column}
     	\begin{column}[T]{7cm} 
         	\begin{center}
		\only<1>{
            		\begin{tikzpicture}[>=stealth', scale = 0.9]
                      % Draw axes
                      \draw [<->,thick] (0,5) node (yaxis) [above] {$y$}
                            |- (5,0) node (xaxis) [right] {$x$};
                      % draw line
                      \draw (0,-1) -- (5,4); % y=x-1
                      \draw[dashed] (-1,0) -- (4,5); % y=x+1
                      \draw[dashed] (2,-1) -- (6,3); % y=x-3
                      % \draw labels
                      \draw (3.5,3) node[rotate=45,font=\small] 
                            {$\mathbf{w}\cdot \mathbf{x} + b = 0$};
                      \draw (2.5,4) node[rotate=45,font=\small] 
                            {$\mathbf{w}\cdot \mathbf{x} + b = 1$};
                      \draw (4.5,2) node[rotate=45,font=\small] 
                            {$\mathbf{w}\cdot \mathbf{x} + b = -1$};
                      % draw distance
                      \draw[dotted] (4,5) -- (6,3);
                      \draw (5.25,4.25) node[rotate=-45] {$\frac{2}{\Vert \mathbf{w} \Vert}$};
                      \draw[dotted] (0,0) -- (0.5,-0.5);
                      \draw (0,-0.5) node[rotate=-45] {$\frac{b}{\Vert \mathbf{w} \Vert}$};
                      \draw[->] (2,1) -- (1.5,1.5);
                      \draw (1.85,1.35) node[rotate=-45] {$\mathbf{w}$};
                      % draw negative dots
                      \fill[red] (0.5,1.5) circle (3pt);
                      \fill[red]   (1.5,2.5)   circle (3pt);
                      \fill[black] (1,2.5)     circle (3pt);
                      \fill[black] (0.75,2)    circle (3pt);
                      \fill[black] (0.6,1.9)   circle (3pt);
                      \fill[black] (0.77, 2.5) circle (3pt);
                      \fill[black] (1.5,3)     circle (3pt);
                      \fill[black] (1.3,3.3)   circle (3pt);
                      \fill[black] (0.6,3.2)   circle (3pt);
                      % draw positive dots
                      \draw[red,thick] (4,1)     circle (3pt); 
                      \draw[red,thick] (3.3,.3)  circle (3pt); 
                      \draw[black]     (4.5,1.2) circle (3pt); 
                      \draw[black]     (4.5,.5)  circle (3pt); 
                      \draw[black]     (3.9,.7)  circle (3pt); 
                      \draw[black]     (5,1)     circle (3pt); 
                      \draw[black]     (3.5,.2)  circle (3pt); 
                      \draw[black]     (4,.3)    circle (3pt); 
                    \end{tikzpicture}}
 %%%%%%%%%%%%%                  
                    \only<2>{
                    \vspace{1cm}
                    \begin{tikzpicture}[>=stealth',x=1cm,y=1cm, scale = .5]
 
                        %draw[color=gray] (0,0) grid (6,6);
                        \draw (0,0) rectangle (6,6);
                        % \draw line
                        \draw[color=red,line width=2pt]
                          (2,6) .. controls (3,5.5) and (3,5) .. 
                          (3,5) .. controls (3,4) and (2,2.5) .. 
                          (2,2) .. controls (2,1) and (2.8,1) .. 
                          (3,1) .. controls (3.5,1) and (3.5,2) .. 
                          (4,2) .. controls (4.5,2) and (6,0) .. 
                          (6,0);
                        % \draw left dashed line
                        \draw[dashed] 
                          (1.5,6) .. controls (2.5,5.5) and (2.5,5) .. 
                          (2.5,5) .. controls (2.5,4) and (1.5,2.5) .. 
                          (1.5,2) .. controls (1.5,.5) and (2.8,.5) .. 
                          (3,.5) .. controls (3.75,.5) and (3.5,1.5) .. 
                          (4,1.5) .. controls (4.5,1.5) and (5.5,0) .. 
                          (5.5,0);
                        % \draw right dashed line
                        \draw[dashed] 
                          (2.5,6) .. controls (3.5,5.5) and (3.5,5) .. 
                          (3.5,5) .. controls (3.5,4) and (2.5,2.5) .. 
                          (2.5,2) .. controls (2.5,1.5) and (2.8,1.5) .. 
                          (3,1.5) .. controls (3.25,1.5) and (3.5,2.5) .. 
                          (4,2.5) .. controls (4.5,2.5) and (6,0.5) .. 
                          (6,0.5);
                        %\draw[color=gray] (2,6) -- (3,5) -- (2,2) -- (3,1) -- (4,2) -- (6,0);
                        %\draw[color=gray] (1.5,6) -- (2.5,5) -- (1.5,2) -- (3,.5)-- (4,1.5)-- (5.5,0);
                        %\draw[color=gray] (2.5,6) -- (3.5,5) -- (2.5,2) -- (3,1.5)-- (4,2.5)-- (6,0.5);
                         
                        %\draw[color=gray] (7,0) grid (13,6);
                        \draw (7,0) rectangle (13,6);
                        % \draw line
                        \draw[color=red,line width=2pt] (8.5,6) -- (12,0);
                        % \draw dashed line
                        \draw[dashed]  (8,6) -- (11.5,0);
                        \draw[dashed]  (9,6) -- (12.5,0);
                         
                        \draw[->,thick] (5,3) -- (8,3) node [above,pos=.5] {$\phi$};
                         
                        \def\positive{{%
                        {2.3,5.3},
                        {3.5,.7},
                        {1.5,2},
                        {1.2,2.1},
                        {1.8,.8},
                        {1,5.5},
                        {1.2,5.8},
                        {.75,.2},
                        {2,4},
                        {5, 0.5},
                        {1.5,3},
                        {2.3,.5},
                        %
                        {9.3,3.3},
                        {11,.8},
                        {8.5,2},
                        {7.2,4.1},
                        {8.8,.8},
                        {8,5.5},
                        {8.2,5},
                        {7.75,.2},
                        {9,4.2},
                        {12, 0.5},
                        {8.5,3},
                        {9.3,.5},
                        }}
                         
                        % \draw positive dots
                        \foreach \i in {0,...,20} {
                          \pgfmathparse{\positive[\i][0]}\let \x \pgfmathresult;
                          \pgfmathparse{\positive[\i][1]}\let \y \pgfmathresult;
                          \fill[black] (\x,\y) circle (2pt);
                        }
                         
                        \def\negative{{%
                        {4,2.5},
                        {3.5,5},
                        {2.6,1.6},
                        {4.5,5.2},
                        {5.5,3.7},
                        {3.9,4.7},
                        {5,2.7},
                        {3.5,4.2},
                        {5.8,.9},
                        %
                        {10.75,3},
                        {10.5,5},
                        {11.6,1.6},
                        {11.5,5.2},
                        {12.5,3.7},
                        {10.9,4.7},
                        {12,2.7},
                        {10.5,4.2},
                        {12.8,.9},
                        }}
                         
                        % \draw negative dots
                        \foreach \i in {0,...,16} {
                          \pgfmathparse{\negative[\i][0]}\let \x \pgfmathresult;
                          \pgfmathparse{\negative[\i][1]}\let \y \pgfmathresult;
                          \draw[black] (\x,\y) circle (3pt);
                        }
                         
                        \end{tikzpicture}
                    }
        		\end{center}
     \end{column}
 \end{columns}
}

\newpage
%\subsection{Further Outlook}

\section{Unsupervised Learning}
\lehrtext{As opposed to supervised learning, the unsupervised algorithms detect patterns in the data on their own. This is very useful especially for higher dimensional cases, where the human eye does not detect them easily.}
%\frame{\frametitle{Machine Learning}
%\framesubtitle{Using MIT OpenCourseware Slides}
%Source:
%\begin{center}
%Eric Grimson, John Guttag, and Ana Bell. 6.0002 Introduction to Computational Thinking and Data Science. Fall 2016. Massachusetts Institute of Technology: MIT OpenCourseWare, https://ocw.mit.edu. License: Creative Commons BY-NC-SA.
%\end{center}
%}
%\setbeamercolor{background canvas}{bg=}
%%\frame{
%\includepdf[page = 4-6]{OCW/ML.pdf}%}

\frame{\frametitle{Clustering}
\lehrtext{For clustering, two fundamental approaches exist:
	\begin{itemize}
		\item Similarity based
		\item Distance based
	\end{itemize}
For both types, the number of clusters $k$ needs to be defined \textit{a priori}.}
\begin{columns}[t] 
     \begin{column}[T]{7cm} 
     \textbf{Hierarchical clustering}
     	\begin{enumerate}
     		\item Assign each item to one cluster
		\item Find closest (most similar cluster): merge them
		\item Continue until desired number of clusters $k$ is reached
     	\end{enumerate}
     \end{column}
     	\begin{column}[T]{7cm} 
         	\textbf{K-means clustering}
     	\begin{enumerate}
		\item Randomly choose $k$ initial centroids
		\item While centroids don't change
		\begin{enumerate}
		\item[E:] Create $k$ clusters by assigning each item to closest centroid
		\item[M:] Compute $k$ new centroids by averaging items in each cluster
		\end{enumerate}
     	\end{enumerate}
     \end{column}
 \end{columns}
 
\begin{center}
 	\includegraphics[width = 12cm]{expectation-maximization}
\end{center}
}

\newpage
\frame{\frametitle{K-means clustering procedure}
\lehrtext{The images below depict an average clustering situation, with appropriate number of clusters and sufficiently random initialisation.}
         	\begin{center}
		\begin{tikzpicture}
            		\node at (0,0) {\includegraphics[width=4.9cm]{PCAprocedure0}};
			\node at (4,0) {\includegraphics[width=4.9cm]{PCAprocedure1}};
			\node at (8,0) {\includegraphics[width=4.9cm]{PCAprocedure2}};
			\node at (0,-3.5) {\includegraphics[width=4.9cm]{PCAprocedure3}};
			\node at (4,-3.5) {\includegraphics[width=4.9cm]{PCAprocedure4}};
			\node at (8,-3.5) {\includegraphics[width=4.9cm]{PCAprocedure5}};
			%\only<7>{\includegraphics[width=4.9cm]{PCAprocedure6}};
		\end{tikzpicture}
        		\end{center}}
		
\frame{\frametitle{K-means inappropriate cluster number}
\lehrtext{In case an inappropriate number of clusters is selected, the algorithms obviously cannot converge to an optimum solution. The example below shows such a case.}
         	\begin{center}
		\begin{tikzpicture}
             		\node at (0,0) {\includegraphics[width=4.9cm]{PCAprocedureFewCluster0}};
			\node at (4,0) {\includegraphics[width=4.9cm]{PCAprocedureFewCluster1}};
			\node at (8,0) {\includegraphics[width=4.9cm]{PCAprocedureFewCluster2}};
			\node at (0,-3.5) {\includegraphics[width=4.9cm]{PCAprocedureFewCluster3}};
			\node at (4,-3.5) {\includegraphics[width=4.9cm]{PCAprocedureFewCluster4}};
			\node at (8,-3.5) {\includegraphics[width=4.9cm]{PCAprocedureFewCluster5}};
			%\only<7>{\includegraphics[width=4.9cm]{PCAprocedure6}};
		\end{tikzpicture}
        		\end{center}}

\newpage
\frame{\frametitle{K-means suboptimal initialisation}
\lehrtext{In case of suboptimal initialisation, the converge of the algorithm is slower or cannot be guaranteed at all, as illustrated below.}
         	\begin{center}
		\begin{tikzpicture}
             		\node at (0,0) {\includegraphics[width=3cm]{PCAprocedureBadInit0}};
			\node at (3,0) {\includegraphics[width=3cm]{PCAprocedureBadInit1}};
			\node at (6,0) {\includegraphics[width=3cm]{PCAprocedureBadInit2}};
			\node at (9,0) {\includegraphics[width=3cm]{PCAprocedureBadInit3}};
			\node at (0,-2.5) {\includegraphics[width=3cm]{PCAprocedureBadInit4}};
			\node at (3,-2.5) {\includegraphics[width=3cm]{PCAprocedureBadInit5}};
			\node at (6,-2.5) {\includegraphics[width=3cm]{PCAprocedureBadInit6}};
			\node at (9,-2.5) {\includegraphics[width=3cm]{PCAprocedureBadInit7}};
			\node at (0,-5) {\includegraphics[width=3cm]{PCAprocedureBadInit8}};
			\node at (3,-5) {\includegraphics[width=3cm]{PCAprocedureBadInit9}};
			\node at (6,-5) {\includegraphics[width=3cm]{PCAprocedureBadInit10}};
			\node at (9,-5) {\includegraphics[width=3cm]{PCAprocedureBadInit11}};
			\end{tikzpicture}
        		\end{center}}

\newpage

\section{Practical hints}
\frame{\frametitle{Practical hints for application of data science methods}
\lehrtext{Try to keep these in mind when approaching a dataset.}
\begin{columns}[t] 
     \begin{column}[T]{6cm} 
     	\begin{itemize}
     		\item Get to know your data
		\item Use training and validation sets
		\item For clustering and ML: 
		\begin{itemize}
		\item Regularise
		\item Remove outliers, NaN
		\end{itemize} 
		\item When building models:
		\begin{itemize}
		\item Don't expect perfect fit
		\item Inspect confusion matrix
		\end{itemize}
     	\end{itemize}
     \end{column}
     	\begin{column}[T]{6cm} 
         	\begin{center}
            		\renewcommand\arraystretch{1.5}
                            \setlength\tabcolsep{0pt}
                            \begin{tabular}{c >{\bfseries}r @{\hspace{0.7em}}c @{\hspace{0.4em}}c @{\hspace{0.7em}}l}
                              \multirow{10}{*}{\parbox{1.1cm}{\bfseries\raggedleft actual\\ value}} & 
                                & \multicolumn{2}{c}{\bfseries Prediction outcome} & \\
                              & & \bfseries p & \bfseries n & \bfseries total \\
                              & p$'$ & \MyBox{True}{Positive} & \MyBox{False}{Negative} & P$'$ \\[2.4em]
                              & n$'$ & \MyBox{False}{Positive} & \MyBox{True}{Negative} & N$'$ \\
                              & total & P & N &
                            \end{tabular}        		
                            \end{center}
     \end{column}
 \end{columns}
}

%\frame{\frametitle{Get to work!}
%\framesubtitle{Load and try \texttt{2-SVM-and-KMeans.ipynb} }
%\begin{center}
%\includegraphics[width = .3\textwidth]{Wagenmeister.png}
%\end{center}
%}

